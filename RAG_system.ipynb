{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLi09z_3qW8O"
      },
      "source": [
        "# RAG System: Analyze Articles with LLMs - NLP Project 2023/2024\n",
        "\n",
        "<table bgcolor=\"#383838\">\n",
        "  <tr height=\"0\" bgcolor=\"#383838\">\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"2\" face=\"Geneva\">Student:</font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"4\"><strong>Lorenzo Carisi</strong></font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"4\"><strong>Lorenzo Lambertini</strong></font></td>\n",
        "  </tr>\n",
        "  <tr height=\"0\" bgcolor=\"#383838\">\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"2\" face=\"Geneva\">Badge ID:</font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"3\"><strong>2085731</strong></font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"3\"><strong>2104288</strong></font></td>\n",
        "  </tr>\n",
        "  <tr height=\"0\" bgcolor=\"#383838\">\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"2\" face=\"Geneva\">Master's Degree Program:</font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"2\"><strong>Computer Engineering - AI & Robotics</strong></font></td>\n",
        "    <td width=\"250\" bgcolor=\"#383838\"><font size=\"2\"><strong>Computer Engineering - AI & Robotics</strong></font></td>\n",
        "  </tr>\n",
        "  <tr height=\"200\" bgcolor=\"#383838\"> <!-- Adjusted height -->\n",
        "    <td colspan=\"3\" bgcolor=\"#383838\">\n",
        "      <img src=\"https://drive.google.com/uc?export=view&id=1xYKfjRKEluKN72IrpiV_JQXsxOxN9TdY\" alt=\"Image\" style=\"width: 100%; max-width: 400px;\">\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyESvl_kisTr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 1 - Domain and Dataset **Profiling**\n",
        "\n",
        "> Domain and dataset should be described in a special section. Dataset profiling (summary of your dataset through descriptive statistics) is also required.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeygpbAN3OBn"
      },
      "source": [
        "The purpose of this project is to devise a `Retrieval Augmented Generation (RAG) framework` able to help people and researchers explore a specific topic through scientific literature.\n",
        "\n",
        "The first phase involves finding a set of articles that summarize our research. To do this, we have employed the services of the free [`Connected Papers platform`](https://www.connectedpapers.com/). After a careful selection of an input paper that introduces the topic, this platform, powered by papers' citations, creates a bibliography with a list of relevant articles linked to our source.\n",
        "\n",
        "> In this instance, our focus is on data augmentation using general segmentation models. We've selected the paper *Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model by Yizhe Zhang et al.*, which can be accessed at [this link](https://arxiv.org/abs/2304.11332). The bibliography generated by the Connected Papers platform has been stored in our university's Google Drive Storage to facilitate accessibility and avoid download issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pne-_YHNMUiR"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    requests==2.31.0 \\\n",
        "    tiktoken==0.6.0 \\\n",
        "    langchain==0.2.1 \\\n",
        "    peft==0.4.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    bitsandbytes==0.40.2 \\\n",
        "    trl==0.4.7 \\\n",
        "    \"safetensors>=0.3.1\" \\\n",
        "    faiss-cpu==1.8.0 \\\n",
        "    sentence-transformers==2.2.2 \\\n",
        "    transformers==4.31.0 \\\n",
        "    langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqyWdB_3kuiO"
      },
      "source": [
        "### **Downloading** PDF Files\n",
        "Next, we proceed getting the articles PDFs from the bibliography.\n",
        "\n",
        "In case of downloading issues, here you can find the [BibTeX file](https://drive.google.com/file/d/1QWnptbhJ3pB7fjjWiulbb3h2sYyl0dxz/view?usp=drive_link) and the relative [scientific articles](https://drive.google.com/file/d/1X7EVqEotuQ48v9qQ74nD63AmKlHzeCRS/view?usp=drive_link)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHA3kx26_kDn"
      },
      "outputs": [],
      "source": [
        "# Downloading the public bibliography file from our university's Google Drive storage account\n",
        "!wget -O /content/list_of_papers.bib \"https://drive.google.com/uc?export=download&id=1QWnptbhJ3pB7fjjWiulbb3h2sYyl0dxz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fI6_1nemGqA"
      },
      "source": [
        "The following snippet of code provides a couple of helper functions for the extraction and downloading of academic papers referenced in the BibTeX file.\n",
        "\n",
        "The first function, `extract_arxiv_ids_and_reference_keys`, parses the BibTeX file to retrieve arXiv IDs and reference keys.\n",
        "\n",
        "The second one, `download_documents_from_bibliography`, leverages these extracted IDs to construct direct download links from arXiv and downloads the corresponding PDF files into a specified folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7gQrrRPzwu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# Function to parse the bibliography and extract arXiv IDs and reference keys\n",
        "def extract_arxiv_ids_and_reference_keys(bibliography_file):\n",
        "    with open(bibliography_file, 'r', encoding='utf-8') as f:\n",
        "        bibliography_content = f.read()\n",
        "\n",
        "    arxiv_ids = re.findall(r'arxivid\\s*=\\s*{([\\d.]+)}', bibliography_content)\n",
        "    reference_keys = re.findall(r'@article\\s*{\\s*([^,]*)', bibliography_content)\n",
        "\n",
        "    return arxiv_ids, reference_keys\n",
        "\n",
        "# Main function to download all documents\n",
        "def download_documents_from_bibliography(bibliography_file, save_folder):\n",
        "    if not os.path.exists(save_folder):\n",
        "        os.makedirs(save_folder)\n",
        "\n",
        "    arxiv_ids, reference_keys = extract_arxiv_ids_and_reference_keys(bibliography_file)\n",
        "\n",
        "    for idx, (arxiv_id, ref_key) in enumerate(zip(arxiv_ids, reference_keys), start=1):\n",
        "        arxiv_link = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "        save_path = os.path.join(save_folder, f\"{ref_key}.pdf\")  # Use reference key as filename\n",
        "        !wget -q -O {save_path} \"{arxiv_link}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFnqwfR1Sjt6"
      },
      "outputs": [],
      "source": [
        "# Download academic papers referenced in the BibTeX file\n",
        "bibliography_file = \"/content/list_of_papers.bib\"\n",
        "save_folder = \"/content/documents\"\n",
        "download_documents_from_bibliography(bibliography_file, save_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m-sqs2GNkz_"
      },
      "source": [
        "### **Loading** Dataset\n",
        "\n",
        "Let's now introducing [LangChain](https://www.langchain.com/), a framework designed to develop advanced applications using large language models (LLMs), that will help us throughout the project by providing a robust suite of tools to streamline the entire process, from initial development to observation and deployment.\n",
        "\n",
        "In order to use the framework, we first need to create a dataset using the articles stored in PDF format, making it compatible with LangChain. For this task we will leverage `langchain_community`, a specialized package designed for integration of third-party applications with LangChain. Moreover we will use `PyPDFLoader`, a tool crafted specifically to deal with PDF files.\n",
        "\n",
        "**Reference:**\n",
        "- [langchain_community](https://pypi.org/project/langchain-community/)\n",
        "- [PyPDFLoader](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygFj5IskOHJT"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain-community==0.2.1 \\\n",
        "    pypdf==4.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqEDO6003ah4"
      },
      "source": [
        "This code snippet define the functions necessary to create a LangChain compatible dataset from the PDF documents.\n",
        "\n",
        "- `clean_pages`: A simple text cleaning function, it removes tabs and extra spaces from the input page content.\n",
        "- `load_pdf`: Loads and cleans PDF pages.\n",
        "- `prepare_pdfs`: Loads all PDFs in a specified folder, aggregating the processed pages into a dataset.\n",
        "\n",
        "Optional debugging output is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynEYxlhJXqyr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "def clean_pages(pages):\n",
        "    for page in pages:\n",
        "        if hasattr(page, 'page_content'):  # Checking if the page has 'page_content' attribute\n",
        "            # Replaces tab characters with a single space\n",
        "            page.page_content = page.page_content.replace('\\t', ' ')\n",
        "            # Replaces multiple spaces with a single space\n",
        "            page.page_content = re.sub(r' +', ' ', page.page_content)\n",
        "    return pages\n",
        "\n",
        "def load_pdf(pdf_path):\n",
        "    # Creates a PyPDFLoader instance for the given PDF path\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    # Loads and splits pages from the provided loader\n",
        "    pages = loader.load_and_split()\n",
        "    # Loads and preprocesses pages from the PDF using the loader\n",
        "    document_pages = clean_pages(pages)\n",
        "    return document_pages\n",
        "\n",
        "def prepare_pdfs(documents_folder, debug=False):\n",
        "    dataset = []  # Initialize dataset as an empty list\n",
        "    if debug:\n",
        "        print(\"Processing PDFs in folder:\", documents_folder)\n",
        "    for filename in os.listdir(documents_folder):\n",
        "        if filename.lower().endswith('.pdf'):  # Checks if the file is a PDF\n",
        "            pdf_path = os.path.join(documents_folder, filename)\n",
        "            if debug:\n",
        "                print(\"Processing PDF:\", pdf_path)\n",
        "            # Processes the single PDF and gets its pages\n",
        "            document_pages = load_pdf(pdf_path)\n",
        "            if debug:\n",
        "                print(\"Number of pages processed:\", len(document_pages))\n",
        "                print(\"Pages added to dataset:\", len(document_pages))\n",
        "            dataset.extend(document_pages)  # Adds the pages to the dataset list\n",
        "    if debug:\n",
        "        print(\"Finished processing all PDFs.\")\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process all PDF files in the \"documents\" folder\n",
        "documents_folder = \"/content/documents\"\n",
        "dataset = prepare_pdfs(documents_folder)"
      ],
      "metadata": {
        "id": "7I5B6ypebPmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ApEMj_moIy4"
      },
      "outputs": [],
      "source": [
        "# Sanity Check: Print the content and metadata of each PDF in the dataset\n",
        "if dataset:\n",
        "    for i, document in enumerate(dataset[:1], start=1):\n",
        "        print(\"***Element\", i, \"first 100 characters of content:***\")\n",
        "        print(document.page_content[:100])  # Print only the first 100 characters of content\n",
        "        print(\"***Metadata:***\")\n",
        "        for key, value in document.metadata.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "        print(\"=\"*50)  # Separate each element's content and metadata\n",
        "    if len(dataset) > 1:\n",
        "        print(f\"Only the first element is printed. Total Elements: {len(dataset)}\")\n",
        "else:\n",
        "    print(\"No element files found in the documents folder.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-JB5ElzZ37h"
      },
      "source": [
        "Store the resulting dataset in CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8nS5YYYRX0b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "# Iterate over each document\n",
        "for document in dataset:\n",
        "    # Extract metadata and page content\n",
        "    metadata = document.metadata\n",
        "    page_content = document.page_content\n",
        "\n",
        "    # Construct a dictionary representing a row\n",
        "    row_dict = {}\n",
        "\n",
        "    # Add metadata values to the row dictionary\n",
        "    row_dict['Source'] = metadata.get('source', '')  # Get source, defaulting to empty string if not present\n",
        "    row_dict['Page'] = metadata.get('page', '')  # Get page, defaulting to empty string if not present\n",
        "\n",
        "    # Add page content to the row dictionary\n",
        "    row_dict['Page_content'] = page_content\n",
        "\n",
        "    # Append the row dictionary to the data list\n",
        "    data.append(row_dict)\n",
        "\n",
        "# Create a DataFrame from the data list\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "# Save the DataFrame to a CSV file with an escape character specified\n",
        "df.to_csv(\"db_SAM_papers.csv\", index=False, escapechar='\\\\')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtBD1H8ezNLO"
      },
      "source": [
        "### **Splitting** in Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CTameZZ0r76"
      },
      "source": [
        "Our dataset now consists of a collection (`$list`) of pages (`$langchain_core.documents.base.Document`) from articles centered on the same topic. However, each page possesses a variety of distinct semantic meanings.\n",
        "\n",
        "If we were to input these pages in their entirety into an embedding model, the resulting vectors would represent a diluted mixture of all the semantic contents present in the articles. Consequently, the query might be matched to something not very relatable.\n",
        "\n",
        "To address this issue, we segment the pages into smaller units, called chunks.\n",
        "\n",
        "- **Recursively split by character**: One common chunking technique within the LangChain framework is the `RecursiveCharacterTextSplitter`. This tool allows you to specify a target chunk size in terms of the number of characters. It then attempts to create chunks of that size, while preserving the integrity of paragraphs, sentences, and words. It achieves this by splitting the text at designated special characters, such as `\\n`, prioritizing the preservation of paragraphs, followed by sentences and words, as much as possible.\n",
        "\n",
        "- **Semantic Chunking**: We have no guarantee that the resulting chunks possess one specific semantic, due to the irregularities of the text compared to the fixed chunking size. A more advance technique is Semantic Chunking. This approach aims to capture the discrete semantic elements more effectively, ensuring that each chunk represents a more coherent and specific meaning, optimizing them for retrieval. langChain provides an experimental `SemanticChunker`.\n",
        "\n",
        "- Alternatively, to perform a semantic chunking we can leverage the `semantic-router` library, a versatile tool for fast and efficient semantic matching and categorization of text.\n",
        "\n",
        "**Reference**:\n",
        "- [LangChain Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/) - Main page for splitting tools in Langchain\n",
        "\n",
        "- [Recursively split by character](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/) - Simple LangChain splitting tool\n",
        "- [Semantic Chunking](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/semantic-chunker/) - Advanced LangChain splitting tool\n",
        "\n",
        "- [semantic-router](https://pypi.org/project/semantic-router/) - external library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRiWW7-OucnM"
      },
      "source": [
        "#### 0. Helper Functions\n",
        "The functions used to save resulting chunk statistics and to visualize them are available here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFOjx2JNQSMg"
      },
      "outputs": [],
      "source": [
        "def write_semantic_chunks_info_to_file(semantic_chunks, output_file):\n",
        "    # Define a function to extract the document and page information from metadata\n",
        "    def get_document_and_page_info(metadata):\n",
        "        document = metadata['source']\n",
        "        page = metadata['page']\n",
        "        return document, page\n",
        "\n",
        "    # Sort the semantic_chunks list based on document and page information\n",
        "    sorted_docs_chunks = sorted(semantic_chunks, key=lambda x: get_document_and_page_info(x.metadata))\n",
        "\n",
        "    # Open a text file in write mode\n",
        "    with open(output_file, \"w\") as file:\n",
        "        # Initialize variables to keep track of current document and page\n",
        "        current_document = None\n",
        "        current_page = None\n",
        "        chunk_index = 0\n",
        "\n",
        "        # Define a helper function to write to file\n",
        "        def write_to_file(text):\n",
        "            file.write(text + '\\n')\n",
        "\n",
        "        # Iterate through sorted docs_chunks\n",
        "        for doc in sorted_docs_chunks:\n",
        "            # Extract document and page information from metadata\n",
        "            document, page = get_document_and_page_info(doc.metadata)\n",
        "\n",
        "            # If the document or page changes, print the document and page information\n",
        "            if document != current_document or page != current_page:\n",
        "                if current_document is not None:\n",
        "                    write_to_file(\"\")  # Write newline to file\n",
        "\n",
        "                write_to_file(f\"Document: {document}\")\n",
        "                write_to_file(f\"Page: {page}\")\n",
        "                write_to_file(\"Chunks:\")\n",
        "\n",
        "                # Update current document and page\n",
        "                current_document = document\n",
        "                current_page = page\n",
        "\n",
        "            # Print chunk information\n",
        "            write_to_file(f\"id: {chunk_index} - size: {len(doc.page_content)}\")\n",
        "\n",
        "            # Increment chunk index\n",
        "            chunk_index += 1\n",
        "\n",
        "        # Write the total number of documents to file\n",
        "        write_to_file(\"\")\n",
        "        write_to_file(\"Total number of documents: \" + str(len(sorted_docs_chunks)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ADjXPEnahNp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def visualize_chunk_distribution(file_path):\n",
        "    # Data parsing and organization\n",
        "    data = {}\n",
        "\n",
        "    # Reading the file\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        current_document = None\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"Document:\"):\n",
        "                current_document = line.split()[-1].split(\"/\")[-1][:6]  # Extracting the first 6 characters after the last /\n",
        "                if current_document not in data:\n",
        "                    data[current_document] = {}\n",
        "            elif line.startswith(\"Page:\"):\n",
        "                current_page = int(line.split()[-1])\n",
        "                data[current_document][current_page] = 0\n",
        "            elif line.startswith(\"id:\"):\n",
        "                data[current_document][current_page] = data[current_document].get(current_page, 0) + 1\n",
        "\n",
        "    # Define function to update the plot based on selected document\n",
        "    def update_plot(change):\n",
        "        document = change['new']\n",
        "        clear_output(wait=True)  # Clear the output display\n",
        "        display(document_dropdown)  # Redisplay the dropdown widget\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        pages = data[document]\n",
        "        page_indices = np.array(list(pages.keys()))\n",
        "        chunks = list(pages.values())\n",
        "        ax.bar(page_indices, chunks, align='center', alpha=0.5)\n",
        "        ax.set_title(\"Chunks Distribution for Document: \" + document)\n",
        "        ax.set_xlabel(\"Page\")\n",
        "        ax.set_ylabel(\"Chunks\")\n",
        "        plt.show()\n",
        "\n",
        "    # Create a dropdown widget to select the document\n",
        "    document_dropdown = widgets.Dropdown(\n",
        "        options=list(data.keys()),\n",
        "        value=list(data.keys())[0],  # Set default value to the first document\n",
        "        description='Document:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Display the dropdown widget\n",
        "    display(document_dropdown)\n",
        "\n",
        "    # Define an event handler to update the plot when the dropdown value changes\n",
        "    document_dropdown.observe(update_plot, names='value')\n",
        "\n",
        "    # Initial plot\n",
        "    update_plot({'new': document_dropdown.value})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnAAmUFiCnJV"
      },
      "outputs": [],
      "source": [
        "def print_splitting_statistics(file_path):\n",
        "    # Data parsing and organization\n",
        "    data = {}\n",
        "\n",
        "    # Variables to track chunk size statistics\n",
        "    min_chunk_size = float('inf')\n",
        "    max_chunk_size = 0\n",
        "    num_chunks = 0\n",
        "\n",
        "    # Reading the file\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        current_document = None\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"Document:\"):\n",
        "                current_document = line.split()[-1].split(\"/\")[-1][:6]  # Extracting the first 6 characters after the last /\n",
        "                if current_document not in data:\n",
        "                    data[current_document] = {}\n",
        "            elif line.startswith(\"Page:\"):\n",
        "                current_page = int(line.split()[-1])\n",
        "                data[current_document][current_page] = 0\n",
        "            elif line.startswith(\"id:\"):\n",
        "                chunk_size = int(line.split()[-1])\n",
        "                # Update min and max chunk size\n",
        "                min_chunk_size = min(min_chunk_size, chunk_size)\n",
        "                max_chunk_size = max(max_chunk_size, chunk_size)\n",
        "                num_chunks = num_chunks + 1\n",
        "\n",
        "    total_documents = len(data.keys())\n",
        "    total_tokens = sum(sum(page.values()) for page in data.values())\n",
        "\n",
        "    print(\"Splitting Statistics:\")\n",
        "    print(f\"  - Total Documents: {total_documents}\")\n",
        "    print(f\"  - Total Splits: {num_chunks}\")\n",
        "    print(f\"  - Minimum Token Size of Split: {min_chunk_size}\")\n",
        "    print(f\"  - Maximum Token Size of Split: {max_chunk_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lcqrz0qKlgn"
      },
      "source": [
        "Helper function to select a sentence transformer model, more on them in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm6lAk60KuUg"
      },
      "outputs": [],
      "source": [
        "def choose_model(model_names):\n",
        "    # Display options to the user\n",
        "    print(\"Please choose a model:\")\n",
        "    for idx, name in enumerate(model_names, start=1):\n",
        "        print(f\"{idx}. {name}\")\n",
        "\n",
        "    # Get user's choice\n",
        "    choice = input(\"Enter the number of your choice: \")\n",
        "\n",
        "    # Validate the choice\n",
        "    if choice.isdigit():\n",
        "        choice_idx = int(choice) - 1\n",
        "        if 0 <= choice_idx < len(model_names):\n",
        "            return model_names[choice_idx]\n",
        "\n",
        "    print(\"Invalid choice. Please enter the number corresponding to your choice.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCLU9jmXmH1f"
      },
      "source": [
        "#### 1. LangChain Semantic Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wehd8kMgni9D"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain_experimental==0.0.59 \\\n",
        "    sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxQtOWk2nu9e"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# To display progress bars in notebooks\n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "# For creating and handling chunks with text content and metadata\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "# List of available model names\n",
        "embedder_names = [\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"sentence-transformers/all-MiniLM-L12-v2\",\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"sentence-transformers/distilbert-base-nli-mean-tokens\"\n",
        "]\n",
        "\n",
        "# Choose a model from the list\n",
        "chosen_model = choose_model(embedder_names)\n",
        "if chosen_model:\n",
        "    print(f\"You have chosen the model: {chosen_model}\")\n",
        "else:\n",
        "    print(\"Please run the cell again and choose a valid option.\")\n",
        "\n",
        "model_name = chosen_model\n",
        "\n",
        "# Create a SemanticChunker instance\n",
        "text_splitter = SemanticChunker(\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name),\n",
        "    #breakpoint_threshold_type = 'percentile',\n",
        "    #breakpoint_threshold_amount = 75,\n",
        "    #number_of_chunks = 2700\n",
        "    )\n",
        "\n",
        "# Split text using SemanticChunker\n",
        "semantic_chunks = text_splitter.transform_documents(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXsUXBxOuAfH"
      },
      "outputs": [],
      "source": [
        "write_semantic_chunks_info_to_file(semantic_chunks, \"/content/sem_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gim5Hb2suQUM"
      },
      "outputs": [],
      "source": [
        "visualize_chunk_distribution(\"/content/sem_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "desZXWYtwyLu"
      },
      "outputs": [],
      "source": [
        "print_splitting_statistics(\"/content/sem_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ8vdSvxS_6T"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Saving for faster future use\n",
        "import pickle\n",
        "\n",
        "with open('semantic_chunks.pkl', 'wb') as f:\n",
        "    pickle.dump(semantic_chunks, f)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVUDbQ78FHD_"
      },
      "source": [
        "#### 2. Recursive Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEgcUVtl00Xm"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512, # the character length of the chunk\n",
        "    chunk_overlap = 64, # the character length of the overlap between chunks\n",
        "    length_function = len, # the length function - in this case, character length (aka the python len() fn.)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9RJUiUD2gS5"
      },
      "outputs": [],
      "source": [
        "rec_chunks = text_splitter.transform_documents(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Jbhbttuw5U"
      },
      "outputs": [],
      "source": [
        "write_semantic_chunks_info_to_file(rec_chunks, \"rec_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLFuz8vOu5JF"
      },
      "outputs": [],
      "source": [
        "visualize_chunk_distribution(\"/content/rec_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUY3oOT3CuYx"
      },
      "outputs": [],
      "source": [
        "print_splitting_statistics(\"/content/rec_chunks_info.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCc98LqnTvA-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Saving for faster future use\n",
        "import pickle\n",
        "\n",
        "with open('rec_chunks.pkl', 'wb') as f:\n",
        "    pickle.dump(rec_chunks, f)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGdtkRBUpv9m"
      },
      "source": [
        "#### 3. Semantic Chunking with `semantic_router`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axh9Y4akDO26"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers==4.31.0 \\\n",
        "    semantic-router==0.0.37 \\\n",
        "    pinecone-client==3.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5XcwChnDkX4"
      },
      "outputs": [],
      "source": [
        "from semantic_router.encoders import HuggingFaceEncoder\n",
        "encoder = HuggingFaceEncoder(model_name=chosen_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XljaEInEFBIX"
      },
      "outputs": [],
      "source": [
        "from semantic_router.splitters import RollingWindowSplitter\n",
        "from semantic_router.utils.logger import logger\n",
        "logger.setLevel(\"WARNING\")  # reduce logs from splitter\n",
        "splitter = RollingWindowSplitter(\n",
        "    encoder=encoder,\n",
        "    dynamic_threshold=True,\n",
        "    min_split_tokens=300,\n",
        "    max_split_tokens=700,\n",
        "    window_size=2,\n",
        "    plot_splits=False,  # set this to true to visualize chunking\n",
        "    enable_statistics=False  # to print chunking stats\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am4c8pzhFGi5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents.base import Document\n",
        "\n",
        "sem_router_chunks = []\n",
        "\n",
        "for document in dataset:\n",
        "    # Collect metadata\n",
        "    metadata = {\n",
        "        \"source\": document.metadata.get(\"source\", \"\"),\n",
        "        \"page\": document.metadata.get(\"page\", \"\")\n",
        "    }\n",
        "\n",
        "    # Merge metadata with page content\n",
        "    content_chunks = splitter([document.page_content])\n",
        "    for content_chunk in content_chunks:\n",
        "        # Create a new Document instance with merged content\n",
        "        new_document = Document(page_content=content_chunk.content, metadata=metadata)\n",
        "        # Append the new document to sem_router_chunks\n",
        "        sem_router_chunks.append(new_document)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpfWScaLUJpN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Saving for faster future use\n",
        "import pickle\n",
        "\n",
        "with open('sem_router_chunks.pkl', 'wb') as f:\n",
        "    pickle.dump(sem_router_chunks, f)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btMj0ZRbnhIm"
      },
      "source": [
        "### Descriptive **statistics**\n",
        "\n",
        "Descriptive statistics, in the context of dataset profiling, refer to the process of summarizing and analyzing key characteristics of a dataset. These statistics provide insights into important features of the data, like the distribution or the central tendency of the data. They commonly include measures such as mean, median, mode and standard deviation, and they are crucial for further analysis and interpretation of the dataset.\n",
        "\n",
        "In the context of a RAG system, performing descriptive statistics on a textual dataset involves analyzing various aspects of the collected text data. It includes summarizing the length of documents, frequency distributions of words or phrases, lexical diversity, and other relevant metrics.\n",
        "\n",
        "To perform such analysis we rely on `ydata-profiling` and `nlp-profiler`.\n",
        "\n",
        "**Reference:**\n",
        "- [ydata-profiling](https://github.com/ydataai/ydata-profiling)\n",
        "- [nlp-profiler](https://github.com/neomatrix369/nlp_profiler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XjcKyhDEeR-n"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ge4wH-leoMP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tTLy2iieqjx"
      },
      "outputs": [],
      "source": [
        "profile = ProfileReport(df, title=\"Profiling Report\")\n",
        "profile.to_widgets()\n",
        "profile.to_file(\"db_profile.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ogMutO6Z5AS"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq nlp_profiler\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ77PgyZaIkP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"..\")\n",
        "\n",
        "from nlp_profiler.core import apply_text_profiling\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzpUmjrNbLoq"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from joblib import Memory\n",
        "# Suppress specific joblib warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*joblib.*\")\n",
        "profiled_text_dataframe = apply_text_profiling(df, \"Page_content\", params={\"parallelisation_method\": \"default\"})\n",
        "profiled_text_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knZacixBIym9"
      },
      "outputs": [],
      "source": [
        "# Save the dataframe to a CSV file\n",
        "csv_file_path = 'profiled_text_dataframe.csv'\n",
        "profiled_text_dataframe.to_csv(csv_file_path, index=False, escapechar='\\\\')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa1gIfhSeC6I"
      },
      "source": [
        "In conclusion, the dataset under examination comprises a collection of 33 academic papers, each contributing to the discourse within a specific domain. These papers vary in length, with the longest spanning 23 pages, resulting in a cumulative total of 454 pages. The dataset has a discrete volume of text of about 1,153,617 characters. This dataset exhibits 210 distinct characters.\n",
        "\n",
        "As expected, the terms \"segmentation\" and \"SAM\" stand out as the most frequently occurring non-trivial words, suggesting their significance within the subject matter under investigation.\n",
        "\n",
        "For more details the entire `profiled_text_dataframe` is available at this [link](https://drive.google.com/uc?export=download&id=16jZ1efNgC0qgKNaPPB1NkF33J6kDYjLk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TVcObs7m5hg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 2 - Dataset **Indexing**\n",
        "\n",
        "> Basic libraries for sentence embedding, vector store, LLM quantization, etc. should all be introduced and briefly documented.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cB0L_CN38W5"
      },
      "source": [
        "Indexing in this context, involves creating data structures such as vector embeddings and metadata to transform structured documents into a resource useful for efficient querying and retrieval within the system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dsC_td9pBqI"
      },
      "source": [
        "### **Selecting** the chunked Dataset\n",
        "To avoid package issues here a section that loads the resulting chunks of the previous section directly from the Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V42kOfS5Wlcv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    os.system(f\"wget -O {output_path} '{url}'\")\n",
        "\n",
        "# Dictionary of files with their download URLs\n",
        "files = {\n",
        "    '1': ('Semantic LangChain - all-MiniLM-L6-v2', 'https://drive.google.com/uc?export=download&id=1KMkomR_s4VvQ9IYR-eL1Z4zTVCVdfcPy'),\n",
        "    '2': ('Recursive LangChain', 'https://drive.google.com/uc?export=download&id=1KEwWCJ0sSsdodf5bi7FtqOo6tR7l-0EL'),\n",
        "    '3': ('Semantic-Router - all-MiniLM-L6-v2', 'https://drive.google.com/uc?export=download&id=1DRxKaZO9p0TiG222X43TQjL6t3bE4iWx'),\n",
        "    '4': ('Current Semantic Chunking LangChain', None),\n",
        "    '5': ('Current Semantic-Router Chunking', None)\n",
        "}\n",
        "\n",
        "# Display options to the user\n",
        "print(\"Please choose which version to use:\")\n",
        "for key, (name, _) in files.items():\n",
        "    print(f\"{key}. {name}\")\n",
        "\n",
        "# Get user's choice\n",
        "choice = input(\"Enter the number of your choice: \")\n",
        "\n",
        "if choice in files:\n",
        "    selected_name, download_url = files[choice]\n",
        "    if download_url:\n",
        "        output_path = f\"/content/chunks.pkl\"\n",
        "        download_file(download_url, output_path)\n",
        "        print(f\"Downloaded {selected_name} to {output_path}\")\n",
        "        # Path to the pickle file\n",
        "        file_path = output_path\n",
        "        # Open the file in read-binary mode and load its contents into a variable\n",
        "        with open(file_path, 'rb') as file:\n",
        "            chunks = pickle.load(file)\n",
        "    else:\n",
        "        if choice == '4':\n",
        "            print(f\"Selected: {selected_name}\")\n",
        "            chunks = semantic_chunks\n",
        "        elif choice == '5':\n",
        "            print(f\"Selected: {selected_name}\")\n",
        "            chunks = sem_router_chunks\n",
        "        else:\n",
        "            print(\"Invalid choice. Please run the cell again and choose a valid option.\")\n",
        "else:\n",
        "    print(\"Invalid choice. Please run the cell again and choose a valid option.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GycdG53N4f9Z"
      },
      "source": [
        "### **Exploring and Implementing** VectorStores\n",
        "\n",
        "In the context of RAG systems, **vector stores** are data structures optimized for efficiently storing and retrieving information. They handle the **embedding** of texts into vectors, including both queries and datasets, and manage the **retrieval** phase by searching for and returning the nearest neighbors to the query.\n",
        "\n",
        "Currently there are a number of different VectorStores, and a number of different strengths and weaknesses to each.\n",
        "\n",
        "In this notebook, we will experiment in the embedding step, with different models. Meanwhile for the vector store architecture we will leverage [Facebook AI Similarity Search](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions.), or `FAISS`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGU96p5R54Xz"
      },
      "source": [
        "The embedding models we will experiment with are the most downloaded on the HuggingFace website:\n",
        "\n",
        "1. **`sentence-transformers/all-MiniLM-L6-v2`**\n",
        "\n",
        "  - This model takes sentences and turns them into numerical representation in a 384-dimensional space.\n",
        "  - It's fast and good for finding similar sentences, fine-tuned on a massive dataset of 1 billion sentence pairs.\n",
        "\n",
        "2. **`sentence-transformers/all-mpnet-base-v2`**\n",
        "  - Similar to `all-MiniLM-L6-v2`, this model takes sentences and turns them into numerical representation in a 768-dimensional space.\n",
        "  - It achieves slightly better accuracy at the cost of being slower, fine-tuned on a massive dataset of 1 billion sentence pairs..\n",
        "\n",
        "3. **`sentence-transformers/all-MiniLM-L12-v2`**\n",
        "  - This model is like a bigger brother to `all-MiniLM-L6-v2`. It uses a more complex architecture and might capture more nuances in sentences, but it's also slower.\n",
        "\n",
        "4. **`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`**\n",
        "  - This model focuses on identifying if sentences mean the same thing, even if phrased differently. It can handle multiple languages and uses a complex MiniLM architecture.\n",
        "  - It also takes sentences and turns them into numerical representation in a 384-dimensional space.\n",
        "\n",
        "5. **`sentence-transformers/distilbert-base-nli-mean-tokens`**\n",
        "  - This model is trained to understand the relationship between sentences, like if they contradict each other.\n",
        "  - It also takes sentences and turns them into numerical representation in a 768-dimensional space.\n",
        "\n",
        "To speed up the performance, we implement a caching mechanism using `CacheBackedEmbeddings`, following the idea of [*Open-Source RAG with Llama 2 and LangChain*](https://www.youtube.com/watch?v=VpbMUSPRJmY). This prevents redundant computation by storing previously computed embeddings and reusing them for similar queries.\n",
        "\n",
        "**Reference:**\n",
        "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
        "- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
        "- [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)\n",
        "- [paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)\n",
        "- [distilbert-base-nli-mean-tokens](https://huggingface.co/sentence-transformers/distilbert-base-nli-mean-tokens)\n",
        "- [CacheBackedEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AOzZWPU05WLr"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "# List of available model names\n",
        "embedder_names = [\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"sentence-transformers/all-MiniLM-L12-v2\",\n",
        "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
        "    \"sentence-transformers/distilbert-base-nli-mean-tokens\"\n",
        "]\n",
        "\n",
        "# Choose a model from the list\n",
        "chosen_model = choose_model(embedder_names)\n",
        "if chosen_model:\n",
        "    print(f\"You have chosen the model: {chosen_model}\")\n",
        "else:\n",
        "    print(\"Please run the cell again and choose a valid option.\")\n",
        "\n",
        "# Specify the embedding model to transform texts into semantic vectors\n",
        "embed_model_id = chosen_model\n",
        "\n",
        "# Initialize the core embedding model using HuggingFace\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "# Set up local file storage for caching to improve performance\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "# Wrap the core embedding model with caching functionality\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "# Create a FAISS vector store and index the dataset using the cached embedding model\n",
        "vector_store = FAISS.from_documents(chunks, embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGHzcE5i6fOR"
      },
      "source": [
        "Now that we've created the VectorStore, we can check that it's working by embedding a query and retrieving chunks from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLOvFNxA6ZSH"
      },
      "outputs": [],
      "source": [
        "query = \"How can SAM be used for data augmentation in medical image segmentation task?\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "context_chunks = vector_store.similarity_search_by_vector(embedding_vector, k = 7)\n",
        "\n",
        "total_character_count = 0\n",
        "\n",
        "# Iterate over the documents and print their content with formatting\n",
        "for i, chunk in enumerate(context_chunks):\n",
        "    content_length = len(chunk.page_content)\n",
        "    total_character_count += content_length\n",
        "    source = chunk.metadata.get('source', 'Unknown source')\n",
        "    source_truncated = source.split(\"/\")[-1][:6]\n",
        "    page_number = chunk.metadata.get('page', 'Unknown page')\n",
        "\n",
        "    # Truncate content if it exceeds 600 characters\n",
        "    if content_length > 600:\n",
        "        chunk_content_display = chunk.page_content[:600] + \"...\"\n",
        "    else:\n",
        "        chunk_content_display = chunk.page_content\n",
        "\n",
        "    print(f\"### Document {i + 1}\")\n",
        "    print(f\"**Source:** {source_truncated}\")\n",
        "    print(f\"**Page Number:** {page_number}\")\n",
        "    print(chunk_content_display)\n",
        "    print(f\"\\n**Character Count:** {content_length}\")\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"**Total Context Character Count:** {total_character_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtT03LCn6cT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 3 - QA **Chain Implementation**\n",
        "\n",
        "> The prompt engineering step should be carefully described in a special section. Design choices should be discussed, including those that have been discarded because of low quality results.\n",
        "\n",
        "Now that the vector store is ready it is time to implement a Retrieval Chain as a bridge between the user and the RAG system.\n",
        "\n",
        "We're going to leverage `return_source_documents=True` to ensure we have proper sources for the system's answers, this way the user can dig further, opening the articles at the page of interest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_waVo3AEk71"
      },
      "source": [
        "### LLM **setup**\n",
        "\n",
        "For what concern the choice of a ChatBot we have decided to try out different possibility and see which one works best for our domain.\n",
        "\n",
        "Specifically, we'll be using:\n",
        "- `meta-llama/Llama-2-13b-chat-hf`\n",
        "- `meta-llama/Meta-Llama-3-8B-Instruct`\n",
        "- `meta-llama/Llama-2-7b-chat-hf`\n",
        "- `nvidia/Llama3-ChatQA-1.5-8B`\n",
        "\n",
        "We will also be leveraging Tim Dettmer's `bitsandbytes` as well as `accelerate` and `transformers` from Hugging Face to make the models smaller, so that they can work in the free version of Google Colab.\n",
        "\n",
        "**Reference:**\n",
        "- [meta-llama/Llama-2-13b-chat-hf](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)\n",
        "- [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
        "- [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n",
        "- [nvidia/Llama3-ChatQA-1.5-8B](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZaXz33mlv4v"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface-hub -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftGCXk7hiDBJ"
      },
      "source": [
        "Login to the Hugging Face platform, if necessary require a token by setting up an account.\n",
        "\n",
        "**Reference:**\n",
        "- [Hugging Face](https://huggingface.co/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNuj7UZNl2A-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face token from Colab's secrets\n",
        "token = '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'\n",
        "\n",
        "# Check if the token is retrieved successfully\n",
        "if token is not None:\n",
        "    # Log in using the retrieved token\n",
        "    notebook_login(new_session=False)\n",
        "else:\n",
        "    print(\"Token not found. Please make sure it's stored in Colab's secrets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfglvbBtExPR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# List of available model names\n",
        "LLM_names = [\n",
        "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
        "]\n",
        "\n",
        "# Call the function and print the chosen model\n",
        "model_id = choose_model(LLM_names)\n",
        "if model_id:\n",
        "    print(f\"You have chosen: {model_id}\")\n",
        "else:\n",
        "    print(\"No valid model chosen.\")\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzl5a1kemBmp"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJX2XhUop5lN"
      },
      "source": [
        "Now we need to pack it into a `pipeline` for compatability with `langchain`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYLhG8_imWei"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    return_full_text=True,\n",
        "    #temperature=5, # It does not work, temperature has to be a strictly positive float\n",
        "    do_sample = False, # Added 30/05/24 Use greedy decoding strategy to always select the token with highest probability\n",
        "    temperature = None, # Added 30/05/24\n",
        "    top_p = None, # Added 30/05/24\n",
        "    max_new_tokens=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-huggingface"
      ],
      "metadata": {
        "id": "MCjGsKS-NWZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1UhO3OTmoOK"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "#from langchain.llms import HuggingFacePipeline #deprecated\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYW8bkFLNx0o"
      },
      "source": [
        "### Retriever **setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGZ4KtHUOwAJ"
      },
      "source": [
        "In this brief section, we specify the retriever options, including the number of documents to retrieve and the criteria for selecting a document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TWJMH8F_w7"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    # search_type=\"similarity_score_threshold\",\n",
        "    # search_kwargs={'score_threshold': 0.01, \"k\": 3},\n",
        "    search_kwargs={\"k\": 4}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNpe9nhaN270"
      },
      "source": [
        "### Template **setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35ZkVEoE8II"
      },
      "source": [
        "We've come to the final phase of setting up the RAG system. Now, we'll design a template with a personalized prompt engineer. Then, we'll integrate all the elements into a unified chain using the tools provided by the `LangChain` framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51OkuY0IRAf8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define a template with custom prompt\n",
        "custom_template = \"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Prompt:\n",
        "You are an AI researcher at a conference. Answer questions from colleagues and students using the Context provided. If Context is empty, say that you don't have enough information to answer.\n",
        "\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance\n",
        "prompt_template = PromptTemplate(\n",
        "    template=custom_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeD8R6huFIf6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "\n",
        "handler = StdOutCallbackHandler()\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    callbacks=[handler],\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\n",
        "        #\"verbose\": True,\n",
        "        #\"prompt\": prompt_template,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMgMdefdoTnN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Section 4 - System **Evaluation**\n",
        "\n",
        "> For this task we do human-based qualitative evaluation. This means that you should test your system on a few questions, analyse possible errors and inaccuracies, and discuss possible ways of improving your system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFv0b0goqAgZ"
      },
      "source": [
        "Now that it's all set-up, we will test the system with some domain specific questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqPaII9nF72R"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What does SAM stand for?\",\n",
        "    \"How can SAM be used for data augmentation in medical image segmentation task?\",\n",
        "    \"During the training process with SAMAug, how is the updating step done?\",\n",
        "    \"Are there any related works to SAMAug?\",\n",
        "    \"What are the most commonly used datasets for polyp segmentation?\",\n",
        "    \"What do you think the next breakthrough in polyp segmentation will be?\"\n",
        "]\n",
        "\n",
        "for i, question in enumerate(questions, start=1):\n",
        "    qa = chain.invoke({\"query\": question})\n",
        "    print(f\"# Question {i}:\")\n",
        "    print(qa['result'])\n",
        "    #with open(\"Output.txt\", \"w\") as text_file:\n",
        "        #text_file.write(qa['result'])\n",
        "    if i != len(questions):\n",
        "        print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best overall performance"
      ],
      "metadata": {
        "id": "tlDhnHgpNuL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "best settings: sentence-transformers/all-MiniLM-L12-v2, Semantic-Router - all-MiniLM-L6-v2, sentence-transformers/all-MiniLM-L12-v2, meta-llama/Llama-2-13b-chat-hf, Temp: 0.0\n",
        "\n",
        "Questions:\n",
        "1. What does SAM stand for?\n",
        "2. How can SAM be used for data augmentation in medical image segmentation task?\n",
        "3. During the training process with SAMAug, how is the updating step done?\n",
        "4. Are there any related works to SAMAug?\n",
        "5. What are the most commonly used datasets for polyp segmentation?\n",
        "6. What do you think the next breakthrough in polyp segmentation will be?\n",
        "\n",
        "Answers:\n",
        "\n",
        "1. SAM stands for Segment Anything Model.\n",
        "\n",
        "2. Colleague: Hi, I'm wondering how SAM can be used for data augmentation in medical image segmentation tasks?  \n",
        "Me: Great question! SAM is a powerful foundation model for segmentation tasks, and it can be used for data augmentation in medical image segmentation tasks. One way to do this is by using the segmentation masks generated by SAM to augment the raw inputs of a medical image segmentation model. This can be done by fusing the segmentation masks with the raw images using a fusion function. This method, called SAMAug, has been proposed in a recent paper and has shown to be effective in boosting the performance of medical image segmentation models.  \n",
        "Student: Can you explain how SAMAug works?  \n",
        "Me: Sure! SAMAug is a simple and novel method that combines segmentation outputs of SAM with raw image inputs to generate SAM-augmented input images for building downstream medical image segmentation models. The fusion function used in SAMAug is a parameter-free module, and the learning process aims to update the parameters of the medical image segmentation model with respect to the given foundation model SAM, the fusion\n",
        "\n",
        "3. The updating step during the training process with SAMAug is done by using the fusion function to combine the raw image inputs with the segmentation outputs of SAM. The parameters of the medical image segmentation model M are updated with respect to the given foundation model SAM, the fusion function, and the training data.\n",
        "\n",
        "4. There are several related works to SAMAug. One of the earliest works is SAM (Self-Attention-based Medical Image Segmentation) [12], which proposed a self-attention mechanism to learn high-level features for medical image segmentation. Since then, there have been several improvements and extensions to SAM, such as SAM-Adapter [6], which improved the segmentation performance of SAM by adding a new branch to the network. Additionally, there have been studies that have used SAM for other tasks, such as inpainting [33], 3D reconstruction [26], and style transformer [17]. Furthermore, there have been several open-source implementations of SAM, such as the Medical Image Segmentation Toolkit (MIST) [28], which provides a user-friendly interface for using SAM on various medical image datasets. Finally, there have been several evaluations of SAM on different datasets, such as the Polyp Region Detection benchmark [8], which showed that SAM outperformed other state-of-the-art methods for polyp segmentation.\n",
        "\n",
        "5. The most commonly used datasets for polyp segmentation are CVC-ClinicDB, CVC-300, and ETIS. These datasets provide a variety of images with different types of polyps, and are widely used in the field to evaluate and compare different segmentation methods. However, it's worth noting that there may be other datasets available as well, depending on the specific research question and application. Do you have any other questions about polyp segmentation or these datasets?\n",
        "\n",
        "6. Well, I think the next breakthrough in polyp segmentation will be the development of more accurate and efficient algorithms that can handle the challenges of real-world clinical settings. For example, there is a growing interest in using deep learning techniques to segment polyps in colonoscopy videos, but there are still many technical challenges to overcome before these methods can be widely adopted in clinical practice. One of the main challenges is the lack of high-quality annotated datasets for training and evaluating these algorithms, which limits their ability to generalize to new cases. Another challenge is the need for more interpretable and explainable methods that can provide insights into the decision-making process of the algorithm. I think the next breakthrough will come from researchers who can develop methods that address these challenges and provide more accurate and efficient polyp segmentation."
      ],
      "metadata": {
        "id": "zTwH1aNXNsZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results discussion"
      ],
      "metadata": {
        "id": "MYOjuFAWCeN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The amount of tests we wanted to do to try out all possible combinations of:\n",
        "- LLM Models\n",
        "- Embedders\n",
        "- Chunking methods\n",
        "- Parameters such as:\n",
        "  - Temperature\n",
        "  - Prompt Templates\n",
        "  - Others\n",
        "\n",
        "It is far higher than the GPU runtime that the Colab environment offers in its free version, even considering a small dataset (around 2k chunks) as the one we have chosen for this project. For this reason, we have limited our analyses to the most crucial aspects, leaving room for further exploration and research in the future.\n",
        "\n",
        "#### **Post-Processing in Chunking**\n",
        "Additionally, it would be worthwhile to investigate the effects of post-processing in the chunking process to eliminate segments that are too small, as they do not contribute significant information.\n",
        "\n",
        "#### **Future Research Directions**\n",
        "Further research may also focus on the prompt engineering stage, employing techniques such as **few-shot learning** and others described [here](https://www.youtube.com/watch?v=RflBcK0oDH0).  \n",
        "We have also thought of **multiquery retrieval** as an alternative way to improve the results, more on this advanced technique at this [link](https://www.youtube.com/watch?v=VFf8XJUIHnU&t=1s).\n",
        "\n",
        "#### **Temperature Adjustment Effects**\n",
        "In our analysis, adjusting the temperature parameter in the `meta-llama/Llama-2-13b-chat-hf` model had minimal effect on the outputs.\n",
        "\n",
        "Other LLMs we tested, particularly LLaMA 3 versions, suffered from prompt incoherence and unfinished answers. Meanwhile, both `meta-llama/Llama-2-13b-chat-hf` and `meta-llama/Llama-2-7b-chat-hf` models demonstrated reliable performance.\n",
        "\n",
        "#### **Detailed Analyses**\n",
        "To see more in-depth analyses, we made a text file available in the main directory of this project.\n",
        "\n",
        "#### **Prompt Engineering Approaches**\n",
        "For the prompt engineering stage, we first left the default template offered by the LangChain framework, and the answers were already very well written.\n",
        "\n",
        "We then tried to modify it to see the difference and switched to a simple:  \n",
        "**\"Answer the provided query.\"**  \n",
        "This more general prompt approach provided answers more decontextualized, yielding hallucinations and severe errors.\n",
        "\n",
        "We then opted for a less general:  \n",
        "**\"Answer the provided query given the context above, if you do not have enough information, just say that you do not know, do not hallucinate.\"**  \n",
        "Although this second approach improved the results, they were still much weaker than the default ones.\n",
        "\n",
        "#### **Iterative Refinement of Prompts**\n",
        "Following iterative refinement, we ultimately developed a highly topic-specific prompt that produced comparable answers in quality to the default settings, which is:  \n",
        "**\"You are an AI researcher at a conference. Answer questions from colleagues and students using the Context provided. If Context is empty, say that you don't have enough information to answer.\"**  \n",
        "\n",
        "#### **Embedding Stage**\n",
        "The embedding stage has been the most straightforward, with `sentence-transformers/all-MiniLM-L12-v2` being the best-performing embedder for all LLM models.\n",
        "\n",
        "In contrast, the last two options available in the ad hoc section did not perform as well, particularly when implementing the chunks using the semantic chunking approach."
      ],
      "metadata": {
        "id": "QBN7IL4-NbTk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQVCkoy3H5Rw"
      },
      "source": [
        "And with that, the project is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAhXlN2dohZq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Section 5 - **Acknowledgment**\n",
        "> If you use solutions that have been inspired by similar projects publicly available in the web, you should add a proper acknowledgement section in your notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyykEFE3oWDW"
      },
      "source": [
        "This Notebook is largly inspired by the one presented in the event put on by [AIMS](https://www.linkedin.com/company/ai-maker-space/), and [Deci](https://deci.ai/), authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/).\n",
        "It can be found at this [link](https://colab.research.google.com/drive/172uMprWwUfEecXQWBrsgDAlkpT_EK39z?usp=sharing).\n",
        "\n",
        "It is also inspired by [James Briggs](https://www.linkedin.com/in/jamescalam/) YouTube video about [Semantic Chunking for RAG](https://www.youtube.com/watch?v=TcRRfcbsApw&t=1396s) and another one about [Prompt Templates for GPT 3.5 and other LLMs - LangChain #2](https://www.youtube.com/watch?v=RflBcK0oDH0)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}